{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Projeto realizado em: <p>2025.1</p>"},{"location":"#kit-d","title":"KIT-D","text":"<p>Jo\u00e3o Vitor Heinke Siqueira</p> <p>Vict\u00f3ria de Oliveira Farias</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>O objetivo deste roteiro \u00e9 detalhar a implementa\u00e7\u00e3o de uma infraestrutura de nuvem utilizando MAAS (Metal as a Service) para configurar servidores f\u00edsicos. O projeto envolve a configura\u00e7\u00e3o de sub-redes, integra\u00e7\u00e3o com APIs, e a implementa\u00e7\u00e3o de uma aplica\u00e7\u00e3o Django na nuvem. O foco \u00e9 automatizar o gerenciamento de hardware, permitindo expans\u00e3o e flexibilidade no deploy de servi\u00e7os.</p>"},{"location":"roteiro1/main/#instalando-o-maas","title":"Instalando o MAAS:","text":"<p>Antes de instalar o MAAS, foi necess\u00e1rio configurar cada NUC com um IP est\u00e1tico e garantir que todos os devices estivessem conectados ao switch e ao roteador fisicamente. O ambiente de rede foi criado para que todas as NUCs pudessesem estabelecer conex\u00e3o e se comunicar entre elas e com a main. Foi utilizado um pendrive para instalar o Ubuntu 22.04 LTS na main MAAS. Ap\u00f3s a instala\u00e7\u00e3o do Ubuntu, O MAAS foi instalado na main utilizando o seguinte comando executado no terminal:</p> sudo snap install maas --channel=3.5/Stable <p>Ap\u00f3s a instala\u00e7\u00e3o da MAAS, implantou-se um banco de dados teste que proporciona um ambiente para testar novas configura\u00e7\u00f5es, permite pr\u00e1ticas seguras sem riscos para a opera\u00e7\u00e3o principal. Al\u00e9m disso, facilita a cria\u00e7\u00e3o de backups e restaura\u00e7\u00f5es r\u00e1pidas, garantindo a estabilidade da infraestrutura enquanto ocorrem novos desenvolvimentos.</p> <p></p> <p>Banco de dados Ativo para o Sistema Operacional</p> <p></p> <p>Banco de dados acess\u00edvel na m\u00e1quina em que foi implementado</p> <p></p> <p>Banco de dados acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN</p> <p></p> <p>Porta em que o Banco de dados est\u00e1 funcionando</p> <p>Para garantir que tudo foi feito com \u00eaxito o dashboard foi acessado atrav\u00e9s do navegador pelo IP configurado (http://172.16.0.3:5240/MAAS) que posteriormente foi atualizado para um novo endere\u00e7o de IP (http://10.103.1.13:5240/MAAS). Essa mudan\u00e7a foi necess\u00e1ria  para que a rede externa (do Insper) reconhecesse a m\u00e1quina principal main como dispositivo ponte para a subrede de NUCs permitindo que a infraestrutura operasse dentro da rede interna e externamente, aumentando a flexibilidade. </p> <p>A configura\u00e7\u00e3o do DNS nessa etapa garantiu que as solicita\u00e7\u00f5es para a nova URL do MAAS fossem direcionadas para o IP atualizado. Comandos de ping para o gateway e para a internet foram usados para testar a conex\u00e3o e garantir que a rede estava configurada corretamente, possibiltando a comunica\u00e7\u00e3o com o mundo externo e entre as m\u00e1quinas. Assim, </p>"},{"location":"roteiro1/main/#configuracao-das-machines","title":"Configura\u00e7\u00e3o das Machines","text":"<p>Cada servidor NUC foi configurado dentro do sistema da MAAS em v\u00e1rias etapas para garantir que cada uma estivesse pronta para ser integrada \u00e0 infraestrutura de nuvem. Para cada servidor, as configura\u00e7\u00f5es de MAC address e IP do AMT foram registradas no dashboard do MAAS. O registro do MAC address identifica cada dispositivo na rede, enquanto o IP do AMT permite o gerenciamento remoto dos servidores, permitindo a\u00e7\u00f5es atrav\u00e9s desse IP dedicado ao AMT. </p> <p>Foram ajustadas as configura\u00e7\u00f5es de rede de cada servidor, incluindo a defini\u00e7\u00e3o de IPs est\u00e1ticos onde necess\u00e1rio, e a configura\u00e7\u00e3o de sub-redes para facilitar a comunica\u00e7\u00e3o entre os servidores e com a rede externa. Assim, para possibilitar acesso remoto, implementou-se um NAT para conectar o servidor principal \"main\" \u00e0 rede Wi-Fi do Insper. Para reduzir a necessidade de duas interfaces de rede f\u00edsicas criou-se cinco OVS bridges na interface 'enp1s0' a partir da configura\u00e7\u00e3o de um n\u00f3 para cada para, desta maneira, garantir que o OVN Chassis possa ser acomodado por qualquer n\u00f3.</p>"},{"location":"roteiro1/main/#django","title":"Django","text":"<p>A implementa\u00e7\u00e3o do Django foi realizada em um dos servidores que j\u00e1 haviam sido provisionados e comissionados com o sistema operacional Ubuntu 22.04 LTS, instalado atrav\u00e9s do MAAS. O Django e as bibliotecas necess\u00e1rias foram instalados, incluindo o Django framework, um adaptador de banco de dados PostgreSQL. O PostgreSQL foi instalado no mesmo servidor onde o Django estava sendo configurado. Ap\u00f3s a configura\u00e7\u00e3o, configurou-se o servi\u00e7o para que ele iniciasse automaticamente com o sistema. As configura\u00e7\u00f5es de listen_addresses no arquivo postgresql.conf foram ajustadas para '0.0.0.0', permitindo que o servidor de banco de dados aceitasse conex\u00f5es de qualquer IP dentro da rede. Regras de firewall foram configuradas para permitir tr\u00e1fego na porta padr\u00e3o do Django (8000) e na porta do PostgreSQL (5432), garantindo que ambos pudessem comunicar-se com outras m\u00e1quinas na rede interna e.</p>"},{"location":"roteiro1/main/#criacao-de-tuneis","title":"Cria\u00e7\u00e3o de t\u00faneis","text":"<p>Para tornar o acesso remoto \u00e0 aplica\u00e7\u00e3o Django mais seguro, foi configurado um t\u00fanel SSH. Essa configura\u00e7\u00e3o permitiu o acesso \u00e0 aplica\u00e7\u00e3o de forma segura, mesmo estando fora do ambiente local. Utilizando este t\u00fanel, todas as solicita\u00e7\u00f5es passam por uma conex\u00e3o criptografada, assegurando que os dados permane\u00e7am protegidos. Utilizando o comando SSH, um t\u00fanel foi estabelecido redirecionando a porta local do computador de um desenvolvedor para a porta do servidor onde o Django estava operando.</p> <p></p> <p>Dashboard do MAAS com as m\u00e1quinas</p> <p></p> <p>imagens do MAAS sincronizadas</p> <p></p> <p>Testes de hardware da server1</p> <p></p> <p>Commissioning da server1 com status \"OK\"</p> <p></p> <p>Testes de hardware da server2</p> <p></p> <p>Commissioning da server2 com status \"OK\"</p> <p></p> <p>Testes de hardware da server3</p> <p></p> <p>Commissioning da server3 com status \"OK\"</p> <p></p> <p>Testes de hardware da server4</p> <p></p> <p>Commissioning da server4 com status \"OK\"</p> <p></p> <p>Testes de hardware da server5</p> <p></p> <p>Commissioning da server5 com status \"OK\"</p>"},{"location":"roteiro1/main/#ansible-e-deploy-automatizado","title":"Ansible e Deploy automatizado","text":"<p>Dashboard do MAAS com as 2 Maquinas e seus respectivos IPs</p> <p></p> <p>Aplicacao Django, provando a conex\u00e3o com o server2</p> <p>O Django e o PostgreSQL foram instalados em servidores separados, iniciando-se com a prepara\u00e7\u00e3o do ambiente. No servidor do Django, um ambiente virtual foi configurado para gerenciar as depend\u00eancias, e o Django foi instalado junto com as bibliotecas necess\u00e1rias. Para o banco de dados, o PostgreSQL foi instalado em seu pr\u00f3prio servidor. O sistema foi configurado para aceitar conex\u00f5es de qualquer IP e foi criado um usu\u00e1rio dedicado para o Django, garantindo uma gest\u00e3o segura e eficiente do banco de dados. As conex\u00f5es foram definidas no arquivo de configura\u00e7\u00e3o do Django para assegurar uma boa comunica\u00e7\u00e3o entre a aplica\u00e7\u00e3o e o banco de dados.</p> <p>Antes de implementar o balanceamento de carga, o deployment das aplica\u00e7\u00f5es Django nos servidores foi automatizado usando Ansible, que melhora a efici\u00eancia e a confiabilidade dos processos de implanta\u00e7\u00e3o. O Ansible faz com que os mesmos comandos podem ser executados v\u00e1rias vezes sem alterar o estado do sistema, permite gerenciar e configurar v\u00e1rias m\u00e1quinas ao mesmo tempo al\u00e9m de ser eficaz na gest\u00e3o de m\u00e1quinas virtuais e cont\u00eaineres.</p> <p>Com os deploys sendo gerenciados automaticamente, asseguramos que as aplica\u00e7\u00f5es Django em server2 e server3 sejam atualizadas e configuradas corretamente. Isso minimiza o tempo de inatividade, mantendo os servi\u00e7os acess\u00edveis e operacionais, mesmo se ocorrer uma falha em um dos n\u00f3s.</p> <p></p> <p>Dashboard do MAAS com as 3 Maquinas e seus respectivos IPs.</p> <p></p> <p>Evidenciando a conex\u00e3o com server2</p> <p></p> <p>Evidenciando a conex\u00e3o com server3</p> <p>Ao utilizar o Ansible para instalar o Django, h\u00e1 um aumento da efici\u00eancia e consist\u00eancia do sistema. Enquanto a instala\u00e7\u00e3o manual est\u00e1 sujeita a erros e varia\u00e7\u00f5es, o Ansible automatiza e padroniza o processo, fazendo com que todas as instala\u00e7\u00f5es sejam feitas da mesma forma em todos os servidores. Isso ajuda na possibilidade de futuras mudan\u00e7as ou atualiza\u00e7\u00f5es, al\u00e9m de ser mais r\u00e1pido, permitindo que altera\u00e7\u00f5es sejam feitas da mesma maneira em toda a infraestrutura.</p>"},{"location":"roteiro1/main/#configuracao-do-load-balancer","title":"Configura\u00e7\u00e3o do Load Balancer","text":"<p>Para garantir efici\u00eancia no tr\u00e1fego de rede para o Django, foi configurado um balanceador de carga usando NGINX no server4, que foi configurado para funcionar como um proxy reverso e balanceador de carga, ou seja, distribuir as solicita\u00e7\u00f5es de rede entre os server2 e server3. Assim, cada nova solicita\u00e7\u00e3o HTTP foi configurada para ser enviada alternadamente entre esses dois servidores.</p> <p></p> <p>Dashboard do MAAS com as 4 Maquinas e seus respectivos IPs</p> <p></p> <p>Server 4 batendo no sever2 em um request</p> <p></p> <p>Server 4 batendo no sever3 em um request seguinte</p> <p>Ap\u00f3s a finaliza\u00e7\u00e3o de todo esse processo, realizou-se o Release de todos os n\u00f3s.</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Os principais desafios inclu\u00edram a configura\u00e7\u00e3o da rede entre os dispositivos, especialmente o ajuste do roteador e do switch para operar corretamente com o MAAS e garantir a comunica\u00e7\u00e3o entre os servidores.</p> <p>A realiza\u00e7\u00e3o deste projeto proporcionou um aprofundamento significativo nos conceitos de rede, gerenciamento de hardware com MAAS e configura\u00e7\u00e3o de aplica\u00e7\u00f5es em um ambiente bare-metal. A experi\u00eancia pr\u00e1tica com estas tecnologias ofereceu um entendimento valioso sobre a automa\u00e7\u00e3o e o gerenciamento de infraestrutura f\u00edsica.</p>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O projeto alcan\u00e7ou com sucesso o objetivo de configurar uma infraestrutura de nuvem bare-metal automatizada, que gerencia hardware f\u00edsico e implanta servi\u00e7os de forma flex\u00edvel e escal\u00e1vel. Esta infraestrutura est\u00e1 preparada para futuras expans\u00f5es ou para ser utilizada em outros projetos que requerem um ambiente robusto e configur\u00e1vel.</p>"},{"location":"roteiro2/main/","title":"Roteiro 2","text":""},{"location":"roteiro2/main/#objetivo","title":"Objetivo","text":"<p>O objetivo deste trabalho foi realizar a orquestra\u00e7\u00e3o de deployment (Deployment Orchestration) de aplica\u00e7\u00f5es distribu\u00eddas (neste caso, Grafana e Prometheus) em uma infraestrutura bare metal, utilizando o MAAS (Metal as a Service) como provedor de m\u00e1quinas f\u00edsicas e o Juju como orquestrador. Durante este trabalho, todo o processo foi realizado com o objetivo de explorar, na pr\u00e1tica, o conceito de Deployment Orchestration, onde conseguimos gerenciar, automatizar e monitorar a aloca\u00e7\u00e3o de recursos e o funcionamento de aplica\u00e7\u00f5es complexas como Grafana e Prometheus em um ambiente real.</p>"},{"location":"roteiro2/main/#instalacao-do-juju","title":"Instala\u00e7\u00e3o do Juju","text":"<p>O primeiro passo foi a instala\u00e7\u00e3o do Juju na m\u00e1quina principal da rede, a main. Essa m\u00e1quina serviu como ponto de partida para repassar comandos e distribui\u00e7\u00f5es \u00e0s NUCs (server1, server2, server3, server4, server5). Utilizou-se o seguinte comando para garantir que a vers\u00e3o mais recente (3.6) fosse instalada corretamente:</p>     sudo snap install juju --channel 3.6"},{"location":"roteiro2/main/#verificacao-e-integracao-com-o-maas","title":"Verifica\u00e7\u00e3o e Integra\u00e7\u00e3o com o MAAS","text":"<p>Foi necess\u00e1rio integrar o Juju com o MAAS, permitindo que ele o reconhecesse como um provedor de cloud. Para verificar se o Juju j\u00e1 reconhecia o MAAS usou-se o comando juju clouds. Como n\u00e3o houve o reconhecimento autom\u00e1tico, foi necess\u00e1rio adicionar manualmente uma defini\u00e7\u00e3o de cloud via o arquivo maas-cloud.yaml: </p> <pre><code>clouds:\n  maas-one:\n    type: maas\n    auth-types: [oauth1]\n    endpoint: http://192.168.0.3:5240/MAAS/\n</code></pre> <p>Esse arquivo informa ao Juju o endpoint do MAAS, al\u00e9m do tipo de autentica\u00e7\u00e3o usada (oauth1). O arquivo foi adicionado ao Juju com o seguinte comando.</p>     juju add-cloud --client -f maas-cloud.yaml maas-one <p>Esse arquivo foi adicionado com</p> juju add-cloud --client -f maas-cloud.yaml maas-one <p>Criou-se o arquivo maas-creds.yaml com a chave OAuth gerada no painel do MAAS para que o Juju possa interagir com a nova cloud adicionada:</p> <pre><code>    credentials:\n        maas-one:\n            anyuser:\n            auth-type: oauth1\n            maas-oauth: &lt;API_KEY&gt;\n</code></pre> <p>Adicionou-se as credenciais com</p> juju add-credential --client -f maas-creds.yaml maas-one"},{"location":"roteiro2/main/#criacao-do-controller","title":"Cria\u00e7\u00e3o do Controller","text":"<p>Para provisionar corretamente o controller em uma m\u00e1quina f\u00edsica: Atribuiu-se a tag juju \u00e0 m\u00e1quina server1 pelo painel do MAAS. Em seguida, executamos o bootstrap do controller com o comando:</p> <pre><code>juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p>Esse processo envolve a aloca\u00e7\u00e3o da m\u00e1quina, instala\u00e7\u00e3o do sistema operacional (Ubuntu), instala\u00e7\u00e3o do Juju Agent e configura\u00e7\u00e3o inicial da cloud privada. O controlador do Juju \u00e9 o componente central que gerencia toda a infraestrutura Juju: ele rastreia os modelos, m\u00e1quinas, aplica\u00e7\u00f5es, estados, etc. </p>"},{"location":"roteiro2/main/#acesso-ao-dashboard","title":"Acesso ao Dashboard","text":"<p>Com o controller funcional, instalou-se o Dashboard do Juju para facilitar a visualiza\u00e7\u00e3o dos deploys e do status das aplica\u00e7\u00f5es. O acesso ao dashboard foi realizado atrav\u00e9s da porta 8080 via um t\u00fanel SSH, o que permitiu operar a interface mesmo estando fora da rede do KIT. Ainda, tamb\u00e9m criou-se uma m\u00e1quina virtual com Ubuntu, usada para auxiliar no controle e nos testes de acesso aos servi\u00e7os.</p>"},{"location":"roteiro2/main/#aplicacao","title":"Aplica\u00e7\u00e3o","text":"<p>O Juju foi essencial para a realiza\u00e7\u00e3o do deploy do Grafana, respons\u00e1vel pela visualiza\u00e7\u00e3o e do Prometheus, respons\u00e1vel pela coleta de m\u00e9tricas. Realizou-se o deploy de ambas as aplica\u00e7\u00f5es. O Grafana \u00e9 uma plataforma de c\u00f3digo aberto que simplifica a apresenta\u00e7\u00e3o visual de dados, como gr\u00e1ficos e pain\u00e9is, facilitando a compreens\u00e3o em tempo real de sistemas e informa\u00e7\u00f5es. Para funcionar, o Grafana requer um banco de dados para armazenar configura\u00e7\u00f5es, metadados e informa\u00e7\u00f5es relacionadas \u00e0 exibi\u00e7\u00e3o de dados em seus pain\u00e9is e gr\u00e1ficos. No caso utilizamos o Prometheus como banco de dados.</p> <p>Para a realiza\u00e7\u00e3o do deploy, primeiro foi necess\u00e1rio criar o diret\u00f3rio charm do Grafana e Prometheus e dentro dele baixar, localmente, os charms das duas aplica\u00e7\u00f5es (Grafana e Prometheus). Um charm \u00e9 um pacote que automatiza a implanta\u00e7\u00e3o, configura\u00e7\u00e3o e gerenciamento de aplica\u00e7\u00f5es em ambientes baseados no Juju. Com os arquivos .charm de ambas as aplica\u00e7\u00f5es, executou-se os deploys para cada um deles, iniciando o processo de instala\u00e7\u00e3o automatizada nas m\u00e1quinas dispon\u00edveis. Ap\u00f3s o deploy ser realizado, foi necess\u00e1rio fazer a integra\u00e7\u00e3o do Grafana com o Prometheus, utilizando um t\u00fanel. A integra\u00e7\u00e3o \u00e9 necess\u00e1ria para que o Grafana reconhe\u00e7a o Prometheus como uma fonte de dados v\u00e1lida.</p> <p>Por fim, realizou-se a limpeza do ambiente com o comando juju destroy-controller, encerrando o controller e liberando os recursos f\u00edsicos alocados no MAAS.</p> <p></p> <p>Dashboard do MAAS com as Maquinas e seus respectivos IPs</p> <p></p> <p>Comando \"juju status\" com o Grafana \"active\"</p> <p></p> <p>Dashboard do Grafana com o Prometheus como source</p> <p></p> <p>Evid\u00eancia do acesso ao Dashboard pela rede do Insper</p> <p></p> <p>Aplica\u00e7\u00f5es sendo gerenciadas pelo JUJU (http://IP-Servi\u00e7o:8080/models/admin/maas)</p>"},{"location":"roteiro2/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O trabalho demonstrou como \u00e9 poss\u00edvel orquestrar aplica\u00e7\u00f5es distribu\u00eddas em uma infraestrutura bare metal utilizando MAAS para o provisionamento de m\u00e1quinas f\u00edsicas e Juju para o deploy automatizado. Ap\u00f3s superar desafios como o recommissionamento da server1 e a configura\u00e7\u00e3o de tags, conseguimos implantar e integrar com sucesso o Grafana e o Prometheus. A integra\u00e7\u00e3o permitiu a visualiza\u00e7\u00e3o das m\u00e9tricas em tempo real, validando a opera\u00e7\u00e3o dos servi\u00e7os. No geral, a atividade mostrou a efici\u00eancia do uso combinado de MAAS e Juju na gest\u00e3o e automa\u00e7\u00e3o de ambientes f\u00edsicos complexos.</p>"},{"location":"roteiro3/main/","title":"Roteiro 3","text":""},{"location":"roteiro3/main/#objetivo","title":"Objetivo","text":"<p>O relat\u00f3rio teve como objetivo documentar o processo de implanta\u00e7\u00e3o e configura\u00e7\u00e3o de uma nuvem privada baseada em OpenStack, utilizando MAAS para provisionamento de hardware e Juju para orquestra\u00e7\u00e3o de charms. Foram apresentados conceitos de infraestrutura (computa\u00e7\u00e3o, rede, armazenamento), deployment de servi\u00e7os essenciais (Ceph, Nova, Keystone, Neutron etc.) e uso da plataforma para cria\u00e7\u00e3o de m\u00e1quinas virtuais e aplica\u00e7\u00f5es.</p>"},{"location":"roteiro3/main/#criacao-da-infraestrutura","title":"Cria\u00e7\u00e3o da Infraestrutura","text":"<p>Criou-se as brigdes em todas os servers e depois colocou-se as tags exigidas em cada uma (controller, reserva, compute, compute, compute). Verificou-se que o OVS bridge br-ex estava presente em todos os n\u00f3s e conectado \u00e0 interface f\u00edsica correta, garantindo a comunica\u00e7\u00e3o externa do OpenStack.</p>"},{"location":"roteiro3/main/#implantacao-do-openstack","title":"Implanta\u00e7\u00e3o do OpenStack","text":""},{"location":"roteiro3/main/#juju-controller","title":"Juju controller","text":"<p>Instalou-se o controller no server1 (que recebeu a tag controller) com o comando:</p> juju bootstrap --bootstrap-series=jammy --constraints tags=controller maas-one maas-controller"},{"location":"roteiro3/main/#definicao-do-modelo-de-deploy","title":"Defini\u00e7\u00e3o do Modelo de Deploy","text":"<p>Definimos o modelo de deploy</p> <p>-- Internamente, o Juju reserva um namespace l\u00f3gico (openstack) para agrupar as aplica\u00e7\u00f5es (charms), m\u00e1quinas virtuais e rela\u00e7\u00f5es que v\u00e3o compor a nuvem OpenStack.</p> <p>-- Muda o contexto de opera\u00e7\u00e3o do seu client Juju para apontar ao model openstack no controller chamado maas-controller.</p> <p>Criou-se e selecionou-se o model openstack sobre o controller rec\u00e9m-bootstrapped:</p> juju add-model --config default-series=jammy openstackjuju switch maas-controller:openstack <p>-- Ap\u00f3s esse comando, todo comando juju status, juju deploy, juju add-machine etc. ser\u00e1 executado no model openstack daquele controller, sem que voc\u00ea precise especificar de novo a qual controller/model est\u00e1 se referindo.</p> <ul> <li>a todo momento, faz-se necess\u00e1rio conferir o status da instalacao do open stack com o comando:</li> </ul> watch -n 2 --color \"juju status --color\" <p>ou a partir de:</p> juju status --color"},{"location":"roteiro3/main/#osd-object-storage-daemon","title":"OSD (Object Storage Daemon)","text":"<p>Cada OSD \u00e9 respons\u00e1vel por gerenciar e servir os dados armazenados em discos locais  - criamos o arquivo ceph-osd.yaml que configura quais discos ser\u00e3o usados como OSDs (Object Storage Daemons) em todos os n\u00f3s.  cria tr\u00eas unidades (units) da aplica\u00e7\u00e3o ceph-osd., aplica as configura\u00e7\u00f5es definidas no arquivo e escala as unidades somente em n\u00f3s \"compute\"</p>"},{"location":"roteiro3/main/#nova-compute","title":"Nova compute","text":"<p>O nova compute \u00e9 o componente do OpenStack respons\u00e1vel por criar e gerenciar as m\u00e1quinas virtuais nos servidores f\u00edsicos.</p>"},{"location":"roteiro3/main/#mysql-innodb-cluster","title":"MYSQL InnoDB Cluster","text":"<p>O MySQL InnoDB Cluster exige sempre, no m\u00ednimo, tr\u00eas r\u00e9plicas de banco de dados. O operador implanta a aplica\u00e7\u00e3o mysql-innodb-cluster em tr\u00eas n\u00f3s, utilizando o charm correspondente. Cada inst\u00e2ncia ser\u00e1 executada dentro de um container LXD nas m\u00e1quinas identificadas como 0, 1 e 2, garantindo a instala\u00e7\u00e3o da vers\u00e3o est\u00e1vel do MySQL. Dessa forma, assegura-se alta disponibilidade e toler\u00e2ncia a falhas, pois o cluster mant\u00e9m tr\u00eas c\u00f3pias sincronizadas dos dados.</p>"},{"location":"roteiro3/main/#vault","title":"Vault","text":"<p>Vault \u00e9 respons\u00e1vel por gerar e gerenciar os certificados TLS que garantem comunica\u00e7\u00e3o criptografada entre os servi\u00e7os da nuvem. Certificado TLS \u00e9 um documento digital que vincula uma chaves criptogr\u00e1fica com uma identidade, garantindo conex\u00e3o ao servidor para um cliente confi\u00e1vel, permite que o cliente e o servidor troquem dados sem possibilidade de intercepta\u00e7\u00e3o externa e assegura integridade dos dados. Para realizar o comando export VAULT_ADRESS usou-se o comando juju status pra identificar em qual m\u00e1quina o Vault teria sido instalado. </p>"},{"location":"roteiro3/main/#neutron-networking","title":"Neutron Networking","text":"<p>O Neutron implementou a rede virtual por meio dos charms neutron-api, neutron-api-plugin-ovn, ovn-central e ovn-chassis. Foi configurado o mapeamento do bridge externo (br-ex) para o provedor plano \u201cphysnet1\u201d e implantadas inst\u00e2ncias dos componentes OVN no cluster LXD. Por fim, todas as rela\u00e7\u00f5es foram estabelecidas para integrar o servi\u00e7o de rede ao plano de controle OVN, \u00e0s m\u00e1quinas de computa\u00e7\u00e3o e ao Vault para certificados TLS, garantindo conectividade e seguran\u00e7a das VMs.</p>"},{"location":"roteiro3/main/#keystone","title":"Keystone","text":"<p>O Keystone centralizou a autentica\u00e7\u00e3o e autoriza\u00e7\u00e3o de usu\u00e1rios, sendo executado em container no n\u00f3 designado. Ele foi conectado ao banco de dados do cluster MySQL e ao Vault para obten\u00e7\u00e3o de certificados TLS, al\u00e9m de fornecer servi\u00e7o de identidade ao Neutron e demais componentes por meio de rela\u00e7\u00f5es diretas, tornando poss\u00edvel o gerenciamento seguro e centralizado de credenciais na nuvem.</p>"},{"location":"roteiro3/main/#rabbitmq","title":"RabbitMQ","text":"<p>Para orquestra\u00e7\u00e3o de mensagens, o broker RabbitMQ foi executado em container no n\u00f3 2. Ap\u00f3s sua instala\u00e7\u00e3o, criaram-se liga\u00e7\u00f5es AMQP tanto com o servi\u00e7o de rede Neutron quanto com o componente de computa\u00e7\u00e3o Nova, permitindo que eventos de rede e inst\u00e2ncia transitassem de forma confi\u00e1vel entre os componentes da nuvem.</p>"},{"location":"roteiro3/main/#nova-cloud-controller","title":"Nova Cloud Controller","text":"<p>O controlador de nuvem Nova, respons\u00e1vel pelo agendamento de inst\u00e2ncias, APIs de computa\u00e7\u00e3o e condu\u00e7\u00e3o de opera\u00e7\u00f5es internas, foi instalado em container no n\u00f3 2. Sua configura\u00e7\u00e3o apontou o Neutron como gestor de rede, integrando-o ao banco de dados e ao RabbitMQ para comunica\u00e7\u00e3o e persist\u00eancia. Em seguida, estabeleceram-se rela\u00e7\u00f5es com Keystone (identidade), Neutron (servi\u00e7o de rede), RabbitMQ (mensageria), Nova Compute (controle de ciclo de vida das VMs) e Vault (certificados), completando o ecossistema de computa\u00e7\u00e3o da nuvem.</p>"},{"location":"roteiro3/main/#placement","title":"Placement","text":"<p>O servi\u00e7o Placement, encarregado de rastrear recursos f\u00edsicos e virtuais dispon\u00edveis para novas inst\u00e2ncias, foi implantado em container no n\u00f3 2. Ele foi conectado ao banco de dados da nuvem, ao Keystone para autoriza\u00e7\u00e3o de identidade e ao controlador Nova para fornecer informa\u00e7\u00f5es sobre aloca\u00e7\u00e3o de recursos, garantindo que a orquestra\u00e7\u00e3o de inst\u00e2ncias levasse em conta a capacidade dispon\u00edvel.</p>"},{"location":"roteiro3/main/#horizon-dashboard-do-openstack","title":"Horizon (Dashboard do OpenStack)","text":"<p>O painel de controle gr\u00e1fico Horizon foi instalado em container no n\u00f3 2, oferecendo interface web unificada para administra\u00e7\u00e3o e uso da nuvem. Ap\u00f3s a implanta\u00e7\u00e3o, o Horizon foi vinculado ao banco de dados e ao Keystone para autentica\u00e7\u00e3o de usu\u00e1rios, permitindo que administradores e usu\u00e1rios acompanhassem facilmente o status de VMs, redes e volumes por meio do browser.</p>"},{"location":"roteiro3/main/#glance","title":"Glance","text":"<p>O servi\u00e7o de imagens Glance foi executado em container no n\u00f3 2, gerenciando o reposit\u00f3rio de imagens de m\u00e1quinas virtuais. Ele foi integrado ao banco de dados, ao Keystone para controle de acesso, ao Vault para certificados e ao Nova Compute para permitir que inst\u00e2ncias fossem inicializadas a partir das imagens armazenadas, al\u00e9m de se conectar ao Ceph como backend de armazenamento para efici\u00eancia e escalabilidade.</p>"},{"location":"roteiro3/main/#ceph-monitor","title":"Ceph Monitor","text":"<p>Para compor o cluster de armazenamento distribu\u00eddo Ceph, o componente de monitoramento (ceph-mon) foi instalado em tr\u00eas containers LXD distribu\u00eddos nos primeiros tr\u00eas n\u00f3s. O monitor foi configurado para reconhecer tr\u00eas OSDs e manter quorum entre si. Posteriormente, estabeleceu-se rela\u00e7\u00f5es entre o monitor e os OSDs, bem como conex\u00f5es ao servi\u00e7o de computa\u00e7\u00e3o Nova e ao Glance, de modo que o Ceph servisse como backend unificado de armazenamento para imagens e discos das VMs.</p>"},{"location":"roteiro3/main/#cinder","title":"Cinder","text":"<p>O servi\u00e7o de blocos Cinder foi implantado em container no n\u00f3 1, configurado para n\u00e3o gerenciar diretamente dispositivos (block-device: None) e usar a API Glance v2. Ele foi integrado ao banco de dados, ao Keystone, ao RabbitMQ e ao Glance para snapshots de volumes, e ao Nova Cloud Controller para prover volumes de inicializa\u00e7\u00e3o. Em seguida, adicionou-se o charm subordinado cinder-ceph, conectando-o ao Ceph Monitor para que os volumes fossem armazenados em RADOS, garantindo alta disponibilidade e desempenho.</p>"},{"location":"roteiro3/main/#ceph-rados-gateway","title":"Ceph RADOS Gateway","text":"<p>Como alternativa ao Swift, o gateway RADOS do Ceph foi instalado em container no n\u00f3 0, oferecendo compatibilidade S3 e Swift. Ap\u00f3s sua implanta\u00e7\u00e3o, ele foi relacionado ao Ceph Monitor, habilitando acesso HTTP aos objetos armazenados, \u00fatil para aplica\u00e7\u00f5es que exigem interface de objeto em nuvem.</p>"},{"location":"roteiro3/main/#integracao-final-do-ceph-osd","title":"Integra\u00e7\u00e3o Final do Ceph-OSD","text":"<p>Ap\u00f3s validar o status est\u00e1vel de todas as unidades OSD, ajustou-se a configura\u00e7\u00e3o do Ceph-OSD para utilizar exclusivamente o dispositivo /dev/sdb, consolidando o layout de armazenamento e otimizando o uso de discos nos n\u00f3s de computa\u00e7\u00e3o.</p>"},{"location":"roteiro3/main/#concluscao-da-implantacao","title":"Conclus\u00e7ao da implanta\u00e7\u00e3o","text":"<p>Aguardou-se a converg\u00eancia do comando de status at\u00e9 que n\u00e3o houvesse mensagens de erro ou pend\u00eancias. A nuvem foi considerada operacional com sucesso. Como pr\u00f3ximos passos, recomendou-se o upgrade dos charms para as vers\u00f5es est\u00e1veis mais recentes, a padroniza\u00e7\u00e3o das s\u00e9ries de sistema operacional em todos os n\u00f3s e a ado\u00e7\u00e3o de pr\u00e1ticas oficiais de backup, recupera\u00e7\u00e3o de desastres e manuten\u00e7\u00e3o cont\u00ednua da plataforma.</p>"},{"location":"roteiro3/main/#configuracao-do-openstack","title":"Configura\u00e7\u00e3o do OpenStack","text":"<p>Exportou-se o arquivo openrc com credenciais de administrador.</p>"},{"location":"roteiro3/main/#autenticacao","title":"Autentica\u00e7\u00e3o","text":"<p>Autenticou-se via:</p>       source ~/openrc"},{"location":"roteiro3/main/#horizon","title":"Horizon","text":"<p>Acessou-se o Horizon pelo browser para acompanhar altera\u00e7\u00f5es em tempo real.</p> <p></p> <p>Juju Status</p> <p></p> <p>Dashboard do MAAS com as m\u00e1quinas</p> <p></p> <p>Compute overview</p> <p></p> <p>Compute instances</p> <p></p> <p>Network topology</p>"},{"location":"roteiro3/main/#imagens-e-flavors","title":"Imagens e Flavors","text":"<p>As imagens representaram o sistema operacional base e qualquer software pr\u00e9-instalado que seria usado para inicializar novas m\u00e1quinas virtuais (VMs). Foi importada uma imagem \u201cJammy\u201d (Ubuntu 22.04) no Glance, o servi\u00e7o de imagens do OpenStack, o qual armazenou o arquivo QCOW2 em seu reposit\u00f3rio. Sempre que uma inst\u00e2ncia foi criada, o Nova buscou essa imagem no Glance e a copiou para o armazenamento do compute, garantindo que todas as VMs partissem de um mesmo template, facilitando a padroniza\u00e7\u00e3o, a atualiza\u00e7\u00e3o de software em massa e a recupera\u00e7\u00e3o r\u00e1pida de sistemas.</p> <p>Instalou-se o cliente OpenStack</p> sudo snap install openstackclients <p>Carregou-se credenciais com</p> source openrc <p>Importou-se imagem Jammy</p> openstack image create --public --container-format bare \\  --disk-format qcow2 --file ~/cloud-images/jammy-amd64.img \\  jammy-amd64 <p>e, por fim, criou-se flavors sem disco ef\u00eamero com os comandos</p> openstack flavor create --ram 1024 --disk 20 m1.tinyopenstack flavor create --ram 2048 --disk 20 m1.smallopenstack flavor create --ram 4096 --disk 20 m1.mediumopenstack flavor create --ram 8192 --disk 20 m1.large <p>Os flavors definiram os perfis de hardware virtual dispon\u00edveis aos usu\u00e1rios, especificando quantidades fixas de CPU, mem\u00f3ria RAM e disco em cada tipo de inst\u00e2ncia. Criaram-se quatro flavors\u2014m1.tiny, m1.small, m1.medium e m1.large\u2014com incrementos graduais de recursos (vCPUs de 1 a 4, RAM de 1 GB a 8 GB e disco de 20 GB). Ao solicitar uma nova VM, o usu\u00e1rio escolheu um flavor adequado \u00e0 carga de trabalho; o Nova ent\u00e3o alocou recursos f\u00edsicos conforme aquele perfil, simplificando o controle de custos e o balanceamento de capacidade no ambiente.</p>"},{"location":"roteiro3/main/#rede-externa","title":"Rede Externa","text":"<p>Configurou-se a rede externa. Usando uma faixa de aloca\u00e7\u00e3o entre 172.16.7.0 e 172.16.8.255</p> openstack network create --external --share \\  --provider-network-type flat --provider-physical-network physnet1 \\  ext_netopenstack subnet create --network ext_net --no-dhcp \\  --gateway 172.16.7.1 --subnet-range 172.16.7.0/24 \\  ext_subnet"},{"location":"roteiro3/main/#rede-interna-e-roteador","title":"Rede Interna e Roteador\u00b6","text":"<p>Criou-se a rede interna e o roteador, usando a subnet 192.169.0.0/24 sem DNS. Na criacao da rede externa e da subrede, foi essencial a cria\u00e7\u00e3o de um roteador pra conect\u00e1-las. O roteador \u00e9 o respons\u00e1vel por encaminhar o tr\u00e1fego entre a rede interna e a externa, permitindo que as inst\u00e2ncias na rede interna acessem a internet e vice-versa. </p> openstack network create --internal user1_netopenstack subnet create --network user1_net \\  --subnet-range 192.169.0.0/24 \\  user1_subnetopenstack router create user1_routeropenstack router add subnet user1_router user1_subnetopenstack router set user1_router --external-gateway ext_net"},{"location":"roteiro3/main/#conexao-ssh-e-security-groups","title":"Conex\u00e3o SSH e Security Groups","text":"<p>Importou-se um par de chaves SSH para o OpenStack, permitindo acesso \u00e0s VMs sem senha, e criou-se um grupo de seguran\u00e7a que liberou somente porta 22 (SSH) e ICMP. Assim, cada inst\u00e2ncia aceitou apenas logins autenticados por chave. Precisa de par de chaves porque quando ocorre a cria\u00e7\u00e3o da maquina virtual, n\u00e3o h\u00e1 permiss\u00e3o para acess\u00e1-la. Assim, o par de chaves insere uma chave publica do MAAS na maquina para que, assim, libere o acesso.</p>"},{"location":"roteiro3/main/#criacao-de-instancias","title":"Cria\u00e7\u00e3o de Inst\u00e2ncias","text":"<p>Em seguida, foi lan\u00e7ada uma nova m\u00e1quina virtual com o menor perfil (m1.tiny), nomeada \u201cclient\u201d. Ap\u00f3s a VM ficar ativa, atribuiu-se um endere\u00e7o flutuante da rede p\u00fablica, permitindo acesso externo via Internet. Esse IP p\u00fablico foi associado \u00e0 interface de rede da inst\u00e2ncia. Por fim, testou-se a conex\u00e3o SSH usando a chave privada local apontada para o usu\u00e1rio padr\u00e3o, confirmando que a m\u00e1quina virtual estava operacional e acess\u00edvel de forma segura de qualquer ponto autorizado na rede.</p>"},{"location":"roteiro3/main/#comparacoes-entre-os-processos","title":"Compara\u00e7\u00f5es entre os processos","text":"<p>Vis\u00e3o Geral de Computa\u00e7\u00e3o</p> <ul> <li>Na Tarefa 1, todas as quotas (inst\u00e2ncias, vCPUs, RAM) estavam zeradas.</li> <li>Na Tarefa 2, j\u00e1 havia 5 inst\u00e2ncias ativas, ocupando 5 vCPUs e 5 GB de RAM.</li> </ul> <p></p> <p>Dashboard do MAAS com as m\u00e1quinas</p> <p></p> <p>Compute overview</p> <p>Inst\u00e2ncias</p> <ul> <li>Antes, n\u00e3o havia nenhuma m\u00e1quina criada.</li> <li>Depois, surgiram cinco VMs (\u201cpostgres\u201d, \u201cnginx\u201d, \u201capi1\u201d, \u201capi2\u201d e \u201cclient\u201d), todas em m1.small exceto \u201cclient\u201d em m1.tiny, cada uma com IP interno e floating IP.</li> </ul> <p></p> <p>Compute instances</p> <p>Topologia de Rede</p> <ul> <li>Inicialmente, s\u00f3 apareciam rede externa e interna ligadas por um roteador.</li> <li>Em seguida, mostraram-se tamb\u00e9m as portas de cada inst\u00e2ncia na rede interna e os \u00edcones de floating IP na rede externa.</li> </ul> <p></p> <p>Network topology</p> <p>Security Groups &amp; Floating IPs</p> <ul> <li>No come\u00e7o, havia apenas o grupo padr\u00e3o com regras b\u00e1sicas (SSH e ICMP) e nenhum IP flutuante.</li> <li>Depois, o grupo ganhou mais regras e todos os cinco floating IPs foram alocados e associados \u00e0s VMs.</li> </ul>"},{"location":"roteiro3/main/#escalonamento-de-nos","title":"Escalonamento de N\u00f3s","text":"<p>Para aumentar a capacidade e a resili\u00eancia da nuvem, foi realizada a integra\u00e7\u00e3o de n\u00f3s adicionais de computa\u00e7\u00e3o e armazenamento. Inicialmente, verificou-se no painel do MAAS a exist\u00eancia de uma m\u00e1quina alocada em reserva; em seguida, liberou-se esse n\u00f3 para uso pelo OpenStack.</p> <p>Em seguida, executou-se o deploy do hypervisor no novo n\u00f3 de compute, adicionando uma unidade ao servi\u00e7o Nova Compute:</p> juju add-unit nova-compute <p>Ap\u00f3s identificar o ID da m\u00e1quina rec\u00e9m-incorporada pelo comando juju status, procedeu-se \u00e0 expans\u00e3o do armazenamento distribu\u00eddo, implantando uma unidade OSD do Ceph naquele mesmo n\u00f3:</p> juju add-unit --to &lt;machine-id&gt; ceph-osd <p>Dessa forma, ganhou-se capacidade de processar mais VMs simult\u00e2neas, distribuir automaticamente cargas de trabalho e melhorar a toler\u00e2ncia a falhas por meio de escalabilidade horizontal.</p> <pre><code>flowchart LR\n    Internet[\"Internet\"] --&gt; Insper[\"Insper&lt;br/&gt;(Firewall / Gateway)\"]\n    Insper --&gt; Machine[\"M\u00e1quina Virtual&lt;br/&gt;(VM)\"]\n    Machine --&gt; OS[\"Keystone, Nova Compute, Neutron, etc.\"]\n    OS --&gt; Instance[\"Inst\u00e2ncias VM\"]\n</code></pre> <p>Mermaid</p>"},{"location":"roteiro3/main/#construcao-e-containerizacao-da-api","title":"Constru\u00e7\u00e3o e Containeriza\u00e7\u00e3o da API","text":"<p>Para conclus\u00e3o do projeto, desenvolveu-se uma API RESTful em FastAPI para gerenciar usu\u00e1rios e consultar dados de terceiros via scraping. A API foi containerizada usando Docker, permitindo f\u00e1cil implanta\u00e7\u00e3o e escalabilidade. O c\u00f3digo fonte da API foi organizado em um reposit\u00f3rio Git, com um Dockerfile para constru\u00e7\u00e3o da imagem e um docker-compose.yml para orquestra\u00e7\u00e3o dos containers. A API oferece os seguintes endpoints:</p> <p>Registrar usu\u00e1rios (POST /registrar).</p> <p>Autenticar usu\u00e1rios (POST /login).</p> <p>Consultar dados de terceiros via scraping (GET /consultar).</p> <p>Health check (GET /health-check) -&gt; evidencia com qual API o load balancer est\u00e1 se comunicando.</p>"},{"location":"roteiro3/main/#aplicacao-do-load-balancer","title":"Aplica\u00e7\u00e3o do Load Balancer","text":"<p>Para garantir alta disponibilidade e balanceamento de carga entre as inst\u00e2ncias da API, foi configurado um load balancer que distribui as requisi\u00e7\u00f5es entre duas inst\u00e2ncias da API. As inst\u00e2ncias da API foram criadas para permitir que m\u00faltiplas inst\u00e2ncias possam ser executadas simultaneamente para lidar com um maior volume de requisi\u00e7\u00f5es. Isso melhora a performance e a resili\u00eancia do sistema, garantindo que, mesmo se uma inst\u00e2ncia falhar, as outras continuem operando e atendendo os usu\u00e1rios. </p> <p>Al\u00e9m disso, foram criadas inst\u00e2ncias do banco de dados PostgreSQL e do load balancer. </p> <pre><code>flowchart TB\n  Internet --&gt; LB[LoadBalancer]\n  LB --&gt; API1[API - VM1]\n  LB --&gt; API2[API - VM2]\n  API1 --&gt; DB[(Postgres - VM Banco)]\n  API2 --&gt; DB\n  User[Cliente] --&gt; Internet</code></pre> <p>Mermaid</p> <p></p> <p>Dashboard do FastAPI</p> <p></p> <p>Demonstra\u00e7\u00e3o do server em que a inst\u00e2ncia da API1 est\u00e1 alocada</p> <p></p> <p>Demonstra\u00e7\u00e3o do server em que a inst\u00e2ncia da API2 est\u00e1 alocada</p> <p></p> <p>Demonstra\u00e7\u00e3o do server em que a inst\u00e2ncia do load balancer est\u00e1 alocada.</p> <p></p> <p>Demonstra\u00e7\u00e3o do server em que a inst\u00e2ncia do banco de dados est\u00e1 alocada.</p>"},{"location":"roteiro3/main/#conclusao","title":"Conclus\u00e3o","text":"<p>A implanta\u00e7\u00e3o do OpenStack com MAAS e Juju demonstrou ser uma solu\u00e7\u00e3o robusta para criar uma nuvem privada escal\u00e1vel e gerenci\u00e1vel. A integra\u00e7\u00e3o de componentes essenciais como Ceph, Nova, Keystone e Neutron permitiu a constru\u00e7\u00e3o de uma infraestrutura de computa\u00e7\u00e3o em nuvem eficiente, capaz de suportar cargas de trabalho variadas. A containeriza\u00e7\u00e3o da API em FastAPI facilitou a gest\u00e3o de usu\u00e1rios e a consulta de dados, enquanto o uso de charms simplificou o processo de orquestra\u00e7\u00e3o e manuten\u00e7\u00e3o dos servi\u00e7os. A experi\u00eancia adquirida neste projeto \u00e9 valiosa para futuras implementa\u00e7\u00f5es e otimiza\u00e7\u00f5es em ambientes de nuvem privada. Demonstrou-se a import\u00e2ncia de:</p> <p>Escolher flavors adequados para custo e desempenho.</p> <p>Seguir boas pr\u00e1ticas de seguran\u00e7a (hash de senhas, vari\u00e1veis de ambiente).</p> <p>Documentar endpoints e automa\u00e7\u00f5es com Diagrama Mermaid.</p> <p>Automatizar tudo via Juju e Docker para reprodutibilidade.</p> <p>O projeto consolida conhecimentos em infraestrutura, DevOps e desenvolvimento Web.</p>"},{"location":"roteiro4/main/","title":"Relat\u00f3rio - Infraestrutura como C\u00f3digo e OpenStack","text":""},{"location":"roteiro4/main/#objetivo","title":"Objetivo","text":"<p>O objetivo deste trabalho foi aplicar os conceitos de Infraestrutura como C\u00f3digo (IaC) utilizando a ferramenta Terraform, al\u00e9m de compreender a estrutura de gerenciamento de projetos no OpenStack. Para isso, realizamos a cria\u00e7\u00e3o hier\u00e1rquica de dom\u00ednios, projetos e usu\u00e1rios no painel Horizon, seguidos do provisionamento de infraestrutura automatizado com Terraform, com a separa\u00e7\u00e3o l\u00f3gica de recursos por aluno.</p>"},{"location":"roteiro4/main/#conceitos-de-infraestrutura-como-codigo-iac","title":"Conceitos de Infraestrutura como C\u00f3digo (IaC)","text":"<p>A Infraestrutura como C\u00f3digo (IaC) permite que a infraestrutura seja definida por meio de arquivos de configura\u00e7\u00e3o leg\u00edveis por humanos, garantindo reprodutibilidade, automa\u00e7\u00e3o e versionamento. O uso de IaC evita configura\u00e7\u00f5es manuais, reduzindo falhas humanas e promovendo ambientes consistentes.</p>"},{"location":"roteiro4/main/#idempotencia","title":"Idempot\u00eancia","text":"<p>Um dos princ\u00edpios-chave do IaC \u00e9 a idempot\u00eancia \u2014 a capacidade de uma opera\u00e7\u00e3o sempre produzir o mesmo resultado independentemente do n\u00famero de vezes que ela \u00e9 executada. Isso garante que o ambiente final sempre estar\u00e1 conforme especificado no c\u00f3digo, independentemente do seu estado anterior.</p>"},{"location":"roteiro4/main/#terraform","title":"Terraform","text":"<p>O Terraform, desenvolvido pela HashiCorp, \u00e9 uma das ferramentas mais populares de IaC. Ele permite definir a infraestrutura em arquivos declarativos e gerenciar todo o seu ciclo de vida com comandos como:</p> <ul> <li>terraform init: inicializa o projeto e instala os plugins necess\u00e1rios.</li> <li>terraform plan: mostra o que ser\u00e1 alterado com base no c\u00f3digo atual.</li> <li>terraform apply: aplica as mudan\u00e7as no ambiente.</li> </ul>"},{"location":"roteiro4/main/#ciclo-de-uso-do-terraform","title":"Ciclo de uso do Terraform:","text":"<ul> <li>Scope: identificar a infraestrutura desejada;</li> <li>Author: escrever a configura\u00e7\u00e3o com c\u00f3digo declarativo;</li> <li>Initialize: <code>terraform init</code> para instalar os plugins;</li> <li>Plan: <code>terraform plan</code> para visualizar mudan\u00e7as;</li> <li>Apply: <code>terraform apply</code> para aplicar mudan\u00e7as planejadas.</li> </ul>"},{"location":"roteiro4/main/#instalacao-no-ubuntu","title":"Instala\u00e7\u00e3o no Ubuntu","text":"<pre><code>  wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\n  echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\n  sudo apt update &amp;&amp; sudo apt install terraform\n</code></pre>"},{"location":"roteiro4/main/#estrutura-de-gestao-de-projetos-no-openstack","title":"Estrutura de Gest\u00e3o de Projetos no OpenStack","text":"<p>O OpenStack fornece uma infraestrutura flex\u00edvel como servi\u00e7o (IaaS). A Canonical oferece ferramentas como:</p> <ul> <li>MAAS: gerenciamento de hardware f\u00edsico (Metal as a Service);</li> <li>JUJU: orquestra\u00e7\u00e3o de servi\u00e7os;</li> <li>OpenStack: gest\u00e3o de recursos de computa\u00e7\u00e3o, rede e armazenamento.</li> </ul> <p>Na aba Identity (Keystone), temos os seguintes elementos:</p> <ul> <li>Domains: unidade de alto n\u00edvel que organiza e isola recursos (ex: departamentos ou clientes).</li> <li>Projects: agrupam recursos de forma isolada para organiza\u00e7\u00e3o, seguran\u00e7a e controle de uso.</li> <li>Users: representam pessoas, sistemas ou servi\u00e7os que interagem com o OpenStack.</li> <li>Groups: conjunto de usu\u00e1rios para facilita\u00e7\u00e3o de permiss\u00f5es.</li> <li>Roles: determinam o que cada usu\u00e1rio pode fazer dentro de um projeto.</li> </ul>"},{"location":"roteiro4/main/#criacao-do-dominio","title":"Cria\u00e7\u00e3o do Dom\u00ednio","text":"<p>Criamos um novo dom\u00ednio chamado AlunosDomain, que ser\u00e1 a unidade superior de organiza\u00e7\u00e3o, isolando os recursos entre diferentes turmas ou times de alunos.</p> <p>Passos:</p> <ul> <li>Acessar: Identity &gt; Domains</li> <li>Criar novo dom\u00ednio: AlunosDomain</li> </ul>"},{"location":"roteiro4/main/#criacao-de-projetos","title":"Cria\u00e7\u00e3o de Projetos","text":"<p>Para cada aluno, criamos um projeto distinto. Os nomes dos projetos seguiram o padr\u00e3o: KitA-NomeAluno.</p> <p>Passos:</p> <ul> <li>Acessar: Identity &gt; Projects</li> <li>Criar projetos como KitA-aluno1 e KitA-aluno2</li> </ul>"},{"location":"roteiro4/main/#criacao-de-usuarios","title":"Cria\u00e7\u00e3o de Usu\u00e1rios","text":"<p>Para cada projeto, foi criado um usu\u00e1rio correspondente com credenciais individuais e papel administrativo.</p> <p>Passos:</p> <ul> <li>Acessar: Identity &gt; Users</li> <li>Criar usu\u00e1rio aluno1 vinculado ao projeto KitA-aluno1 e dom\u00ednio AlunosDomain</li> <li>Criar usu\u00e1rio aluno2 vinculado ao projeto KitA-aluno2</li> <li>Atribuir papel de admin a cada usu\u00e1rio dentro de seu respectivo projeto</li> </ul>"},{"location":"roteiro4/main/#atribuicao-de-papeis-roles","title":"Atribui\u00e7\u00e3o de Pap\u00e9is (Roles)","text":"<p>Caso a atribui\u00e7\u00e3o do papel admin n\u00e3o tenha sido feita na cria\u00e7\u00e3o do usu\u00e1rio, acessamos:</p> <ul> <li>Identity &gt; Projects &gt; [projeto do aluno] &gt; Members &gt; Manage Members</li> <li>Adicionamos o usu\u00e1rio e atribuirmos o papel de admin</li> </ul>"},{"location":"roteiro4/main/#criando-a-infraestrutura-utilizando-iac","title":"Criando a Infraestrutura utilizando IaC","text":""},{"location":"roteiro4/main/#estrutura-de-arquivos","title":"Estrutura de Arquivos","text":"<p>KitA-alunoX/    \u2514\u2500\u2500 terraform/       \u251c\u2500\u2500 provider.tf       \u251c\u2500\u2500 instance1.tf       \u251c\u2500\u2500 instance2.tf       \u251c\u2500\u2500 network.tf       \u2514\u2500\u2500 router.tf</p>"},{"location":"roteiro4/main/#credenciais","title":"Credenciais","text":"<ol> <li>Baixe o RC file do seu usu\u00e1rio acessando: <code>Project &gt; API Access</code>;</li> <li>Copie o arquivo para o servidor e conceda permiss\u00e3o de execu\u00e7\u00e3o com o comando:    <pre><code> chmod +x arquivo.sh\n</code></pre> <ol> <li>Carregue as vari\u00e1veis de ambiente:</li> </ol> </li> </ol> <pre><code>  source arquivo.sh\n</code></pre> <p>Para fazer a implementa\u00e7\u00e3o da infraestrutura, execute os comandos abaixo:</p> <pre><code>  terraform plan\n</code></pre> <p>Este comando \u00e9 utilizado para criar um plano de execu\u00e7\u00e3o. Ele mostra quais a\u00e7\u00f5es o Terraform executar\u00e1 quando voc\u00ea aplicar suas configura\u00e7\u00f5es.</p> <pre><code>  terraform apply\n</code></pre> <p>O comando terraform apply aplica as mudan\u00e7as necess\u00e1rias para alcan\u00e7ar o estado desejado da sua configura\u00e7\u00e3o. Ele cria, atualiza ou destr\u00f3i os recursos conforme necess\u00e1rio.</p> <p> </p>"},{"location":"roteiro4/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O trabalho permitiu aplicar, na pr\u00e1tica, os princ\u00edpios de Infraestrutura como C\u00f3digo, al\u00e9m de organizar o ambiente OpenStack de forma escal\u00e1vel e modular. Criamos uma hierarquia de dom\u00ednio, projetos e usu\u00e1rios separados para cada aluno, refletindo o comportamento de ambientes reais de nuvem.</p> <p>A ferramenta Terraform se mostrou essencial na automa\u00e7\u00e3o e padroniza\u00e7\u00e3o das configura\u00e7\u00f5es, reduzindo a possibilidade de erros manuais e garantindo consist\u00eancia no ambiente.</p> <p>A cria\u00e7\u00e3o dessa infraestrutura com separa\u00e7\u00e3o l\u00f3gica entre os usu\u00e1rios \u00e9 essencial para a escalabilidade e seguran\u00e7a em projetos que simulam ambientes multi-tenant, como os encontrados em empresas e institui\u00e7\u00f5es que utilizam OpenStack.</p>"},{"location":"roteiro4/main/#resposta-perguntas","title":"Resposta perguntas","text":""},{"location":"roteiro4/main/#escolha-entre-public-cloud-ou-private-cloud","title":"Escolha entre Public Cloud ou Private Cloud","text":"<p>Escolha: Private Cloud</p> <p>Motivo: - Maior controle de seguran\u00e7a e compliance. - Dados sigilosos exigem ambiente dedicado. - Menor custo total com tr\u00e1fego e prote\u00e7\u00e3o. - Personaliza\u00e7\u00e3o da infraestrutura. - Integra\u00e7\u00e3o via VPN entre sedes.</p>"},{"location":"roteiro4/main/#por-que-preciso-de-um-time-de-devops-para-rh","title":"Por que preciso de um time de DevOps (para RH)","text":"<p>Fun\u00e7\u00f5es do time DevOps: - Automatizar deploys (CI/CD). - Reduzir erros manuais. - Aumentar a velocidade de entrega. - Garantir estabilidade e escalabilidade. - Infraestrutura como c\u00f3digo (IaC).</p> <p>Benef\u00edcios: - Menor downtime. - Resposta r\u00e1pida a falhas. - Ambientes consistentes.</p>"},{"location":"roteiro4/main/#plano-de-dr-disaster-recovery-e-ha-alta-disponibilidade","title":"Plano de DR (Disaster Recovery) e HA (Alta Disponibilidade)","text":""},{"location":"roteiro4/main/#ameacas-principais","title":"Amea\u00e7as principais","text":"<ul> <li>Falha de hardware.</li> <li>Ataques cibern\u00e9ticos (ransomware).</li> <li>Erro humano.</li> <li>Falta de energia/rede.</li> <li>Desastres naturais.</li> </ul>"},{"location":"roteiro4/main/#acoes-de-recuperacao-dr","title":"A\u00e7\u00f5es de Recupera\u00e7\u00e3o (DR)","text":"<ol> <li>Replica\u00e7\u00e3o geogr\u00e1fica dos dados.</li> <li>Ambientes de conting\u00eancia (hot-standby).</li> <li>Manual de recupera\u00e7\u00e3o e testes regulares.</li> <li>Monitoramento e alarmes autom\u00e1ticos.</li> </ol>"},{"location":"roteiro4/main/#politica-de-backup","title":"Pol\u00edtica de Backup","text":"<ul> <li>Frequ\u00eancia: Di\u00e1rios (incrementais), Semanais (completos).</li> <li>Local: Regi\u00e3o separada, offsite criptografado.</li> <li>Reten\u00e7\u00e3o: 30 dias a 5 anos.</li> <li>Testes: Recupera\u00e7\u00e3o trimestral.</li> <li>Criptografia: AES-256 (em repouso), TLS 1.3 (em tr\u00e2nsito).</li> </ul>"},{"location":"roteiro4/main/#alta-disponibilidade-ha","title":"Alta Disponibilidade (HA)","text":"<ul> <li>Load balancers para distribui\u00e7\u00e3o de carga.</li> <li>Clusters de aplica\u00e7\u00e3o com Kubernetes.</li> <li>Banco de dados replicado com failover autom\u00e1tico.</li> <li>Monitoramento com Prometheus + Grafana.</li> <li>Redund\u00e2ncia de rede, energia e storage.</li> <li>Scripts de self-healing.</li> </ul>"}]}